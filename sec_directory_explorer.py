#!/usr/bin/env python3
"""
SEC Directory Explorer - åˆ†æSEC filingç›®å½•ç»“æ„æ‰¾åˆ°XMLæ–‡ä»¶
"""

import requests
import re
from bs4 import BeautifulSoup
import time

def explore_sec_filing_directory(accession_number, cik="1067983"):
    """
    æ¢ç´¢SEC filingç›®å½•ï¼Œæ‰¾åˆ°æ‰€æœ‰å¯ç”¨æ–‡ä»¶
    """
    print(f"ğŸ” æ¢ç´¢SEC filingç›®å½•: {accession_number}")
    
    session = requests.Session()
    session.headers.update({'User-Agent': 'Investment Research analysis@example.com'})
    
    # SEC filingç›®å½•çš„indexé¡µé¢
    accession_clean = accession_number.replace('-', '')
    index_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_clean}/{accession_number}-index.htm"
    
    try:
        print(f"ğŸ“„ è®¿é—®indexé¡µé¢: {index_url}")
        response = session.get(index_url, timeout=30)
        
        if response.status_code == 200:
            # è§£æHTMLæ‰¾åˆ°æ‰€æœ‰æ–‡ä»¶é“¾æ¥
            soup = BeautifulSoup(response.content, 'html.parser')
            
            print(f"âœ… æˆåŠŸè·å–indexé¡µé¢")
            print(f"ğŸ“„ é¡µé¢å†…å®¹é¢„è§ˆ:")
            print(response.text[:1000])
            print("...")
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶é“¾æ¥
            links = soup.find_all('a', href=True)
            xml_files = []
            all_files = []
            
            for link in links:
                href = link.get('href')
                if href and not href.startswith('http'):
                    filename = href.split('/')[-1]
                    all_files.append(filename)
                    
                    if filename.endswith('.xml'):
                        xml_files.append(filename)
            
            print(f"\nğŸ“ å‘ç°çš„æ‰€æœ‰æ–‡ä»¶ ({len(all_files)}ä¸ª):")
            for i, filename in enumerate(all_files, 1):
                file_type = "ğŸ“„ XML" if filename.endswith('.xml') else "ğŸ“„ å…¶ä»–"
                print(f"  {i:2d}. {file_type} {filename}")
            
            print(f"\nğŸ¯ XMLæ–‡ä»¶ ({len(xml_files)}ä¸ª):")
            for i, filename in enumerate(xml_files, 1):
                print(f"  {i}. {filename}")
            
            return xml_files, all_files
            
        else:
            print(f"âŒ HTTP {response.status_code}")
            
            # å°è¯•alternative URLs
            alt_urls = [
                f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_clean}/",
                f"https://www.sec.gov/ix?doc=/Archives/edgar/data/{cik}/{accession_clean}/{accession_number}.htm"
            ]
            
            for alt_url in alt_urls:
                print(f"ğŸ”„ å°è¯•å¤‡ç”¨URL: {alt_url}")
                alt_response = session.get(alt_url, timeout=30)
                if alt_response.status_code == 200:
                    print(f"âœ… å¤‡ç”¨URLæˆåŠŸ!")
                    print(alt_response.text[:500])
                    break
            
            return [], []
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return [], []

def test_xml_files(accession_number, xml_files, cik="1067983"):
    """
    æµ‹è¯•å‘ç°çš„XMLæ–‡ä»¶ï¼Œçœ‹å“ªä¸ªæ˜¯13Fä¿¡æ¯è¡¨
    """
    if not xml_files:
        print("âš ï¸ æ²¡æœ‰å‘ç°XMLæ–‡ä»¶")
        return None
    
    print(f"\nğŸ§ª æµ‹è¯•XMLæ–‡ä»¶å†…å®¹...")
    
    session = requests.Session()
    session.headers.update({'User-Agent': 'Investment Research analysis@example.com'})
    
    accession_clean = accession_number.replace('-', '')
    found_info_table = None
    
    for filename in xml_files:
        print(f"\nğŸ“„ æµ‹è¯•æ–‡ä»¶: {filename}")
        url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_clean}/{filename}"
        
        try:
            response = session.get(url, timeout=30)
            if response.status_code == 200:
                content = response.text
                
                # æ£€æŸ¥æ˜¯å¦ä¸º13Fä¿¡æ¯è¡¨
                indicators = [
                    'nameofissuer', 'cusip', 'shrsornramt', 'infotable',
                    'informationtable', 'thirteenf', '13f'
                ]
                
                found_indicators = []
                for indicator in indicators:
                    if indicator in content.lower():
                        found_indicators.append(indicator)
                
                print(f"  ğŸ“Š æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦")
                print(f"  ğŸ¯ 13FæŒ‡æ ‡: {found_indicators}")
                
                if len(found_indicators) >= 3:  # å¦‚æœåŒ…å«å¤šä¸ª13FæŒ‡æ ‡
                    print(f"  âœ… å¯èƒ½æ˜¯13Fä¿¡æ¯è¡¨!")
                    found_info_table = filename
                    
                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                    print(f"  ğŸ“„ å†…å®¹é¢„è§ˆ:")
                    print(f"    {content[:300]}...")
                    
                else:
                    print(f"  âŒ ä¸åƒ13Fä¿¡æ¯è¡¨")
                    
            else:
                print(f"  âŒ HTTP {response.status_code}")
                
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
        
        time.sleep(0.1)  # é¿å…è¿‡å¿«è¯·æ±‚
    
    return found_info_table

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¢ç´¢ä¼¯å…‹å¸Œå°”çš„å†å²13F filing
    """
    print("ğŸ¯ SEC 13Få†å²æ–‡ä»¶æ¢ç´¢å™¨")
    print("="*50)
    
    # è¦æ¢ç´¢çš„å†å²filing
    historical_filings = [
        "0000950123-25-002701",  # 2025-02-14
        "0000950123-24-011775",  # 2024-11-14  
        "0000950123-24-008740",  # 2024-08-14
        "0000950123-24-005664",  # 2024-05-15 (amended)
    ]
    
    cik = "1067983"  # ä¼¯å…‹å¸Œå°”
    
    for accession in historical_filings:
        print(f"\n{'='*60}")
        print(f"æ¢ç´¢filing: {accession}")
        print(f"{'='*60}")
        
        # æ­¥éª¤1ï¼šæ¢ç´¢ç›®å½•ç»“æ„
        xml_files, all_files = explore_sec_filing_directory(accession, cik)
        
        # æ­¥éª¤2ï¼šæµ‹è¯•XMLæ–‡ä»¶
        if xml_files:
            info_table_file = test_xml_files(accession, xml_files, cik)
            
            if info_table_file:
                print(f"\nğŸ‰ æ‰¾åˆ°13Fä¿¡æ¯è¡¨: {info_table_file}")
                print(f"å®Œæ•´URL:")
                accession_clean = accession.replace('-', '')
                full_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_clean}/{info_table_file}"
                print(f"  {full_url}")
                
                # è¿™ä¸ªURLæ¨¡å¼å¯ä»¥ç”¨äºæ›´æ–°åˆ†æå™¨
                return accession, info_table_file
        
        print(f"\nâ³ ç­‰å¾…1ç§’é¿å…è¿‡å¿«è¯·æ±‚...")
        time.sleep(1)
    
    print(f"\nâŒ æœªåœ¨ä»»ä½•å†å²filingä¸­æ‰¾åˆ°13Fä¿¡æ¯è¡¨")
    return None, None

if __name__ == "__main__":
    found_accession, found_filename = main()
    
    if found_filename:
        print(f"\nâœ… æˆåŠŸï¼å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å¼æ›´æ–°åˆ†æå™¨:")
        print(f"   æ–‡ä»¶æ¨¡å¼: {found_filename}")
        print(f"   åº”ç”¨åˆ°å…¶ä»–filing")